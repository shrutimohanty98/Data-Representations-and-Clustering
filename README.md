# Data Representations and Clustering

Machine learning algorithms are used to analyze text and images, among other types of data. Before using these techniques, raw data must be converted into feature representations that may be used by downstream algorithms. We looked at feature extraction from text data and categorization as a downstream operation in the previous project. We have discovered that lowering the dimension of the retrieved characteristics might often aid a subsequent task.

We combine the ideas of feature extraction and clustering in this research. In an ideal world, all we'd need are data points encoded with specified attributes, and AI would be able to figure out what's important to learn, or more precisely, what the dataset's underlying modes or categories are. The ultimate goal of General AI is for a machine to be able to build a knowledge base on its own, function as its own teacher, and interact with the outside world to learn and explore in order to be able to operate autonomously in a given environment.

We begin by looking at unsupervised learning using textual data, which is a continuation of what we studied in Project 1. We want to know if a mix of feature engineering and clustering techniques can automatically divide a document set into groups with labels that match.

Next, we'll look at a different type of data: images. We first look at how to obtain image features using "deep learning" or "deep neural networks (DNNs)." Large neural networks have been trained to distinguish objects of various types from photos using large annotated image datasets. Networks trained on the Imagenet dataset, for example, can categorize over a thousand different object categories.The first half of such networks uses convolutional filters to convert a given RGB image into a feature vector, and the second portion uses a fully-connected multi-layered neural network to classify this feature vector into an appropriate category (we will study such NNs in a later lecture). Such pre-trained networks could be thought of as experienced agents who have learned to recognize important elements in images.

Is it possible to leverage such pre-trained agents' experience to understand new images that the machine has never seen before? It's like asking a human forensics expert to investigate a new murder scene. An expert in this field should be able to transfer their domain knowledge to a new situation. Can a pre-trained network for image interpretation be utilized for transfer learning in a similar way? The output of the network in the last few layers could be used as expert features. Then, given a multi-modal dataset containing images from categories for which the DNN was not trained, feature engineering (such as dimensionality reduction) and clustering methods can be used to extract unlabeled categories from expert features.

To compare the groups recovered by the unsupervised learning algorithms to the equivalent ground truth human labels, one can utilize a standard set of multiple assessment criteria for both text and image data.

This project is a part of Winter 2022 Course - Large Scale Data Mining (ECE 219)
